{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac14c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "\n",
    "from train import train \n",
    "from test import test\n",
    "from test_beam import test_beam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270cd592",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24cdd0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreFalseAction(option_strings=['--no-attention'], dest='attention', nargs=0, const=False, default=True, type=None, choices=None, help='Use this for convcap without attention', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Convolutional Image Captioning Model')\n",
    "\n",
    "parser.add_argument('model_dir', help='output directory to save models & results')\n",
    "\n",
    "parser.add_argument('-g', '--gpu', type=int, default=0,\\\n",
    "                    help='gpu device id')\n",
    "\n",
    "parser.add_argument('--coco_root', type=str, default= './data/coco/',\\\n",
    "                    help='directory containing coco dataset train2014, val2014, & annotations')\n",
    "\n",
    "parser.add_argument('-t', '--is_train', type=int, default=1,\\\n",
    "                    help='use 1 to train model')\n",
    "\n",
    "parser.add_argument('-e', '--epochs', type=int, default=30,\\\n",
    "                    help='number of training epochs')\n",
    "\n",
    "parser.add_argument('-b', '--batchsize', type=int, default=20,\\\n",
    "                    help='number of images per training batch')\n",
    "\n",
    "parser.add_argument('-c', '--ncap_per_img', type=int, default=5,\\\n",
    "                    help='ground-truth captions per image in training batch')\n",
    "\n",
    "parser.add_argument('-n', '--num_layers', type=int, default=3,\\\n",
    "                    help='depth of convcap network')\n",
    "\n",
    "parser.add_argument('-m', '--nthreads', type=int, default=4,\\\n",
    "                    help='pytorch data loader threads')\n",
    "\n",
    "# parser.add_argument('-ft', '--finetune_after', type=int, default=8,\\\n",
    "#                     help='epochs after which vgg16 is fine-tuned')\n",
    "\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=5e-5,\\\n",
    "                    help='learning rate for convcap')\n",
    "\n",
    "parser.add_argument('-st', '--lr_step_size', type=int, default=15,\\\n",
    "                    help='epochs to decay learning rate after')\n",
    "\n",
    "parser.add_argument('-sc', '--score_select', type=str, default='CIDEr',\\\n",
    "                    help='metric to pick best model')\n",
    "\n",
    "parser.add_argument('--beam_size', type=int, default=1, \\\n",
    "                    help='beam size to use for test') \n",
    "\n",
    "parser.add_argument('--attention', dest='attention', action='store_true', \\\n",
    "                    help='Use this for convcap with attention (by default set)')\n",
    "\n",
    "parser.add_argument('--no-attention', dest='attention', action='store_false', \\\n",
    "                    help='Use this for convcap without attention')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf0548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.set_defaults(attention=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aeed33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.finetune_after = 8\n",
    "args.model_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b1870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f87f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import numpy as np \n",
    "import json\n",
    "import time\n",
    " \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models                                                                     \n",
    "\n",
    "from coco_loader import coco_loader\n",
    "from convcap import convcap\n",
    "from vggfeats import Vgg16Feats\n",
    "from tqdm import tqdm \n",
    "from test import test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92131ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d80637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "if (args.is_train == 1):\n",
    "    print('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d858dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotation file...\n",
      "Found 113287 images in split: train\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading train data ... 3.618005 secs\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "train_data = coco_loader(args.coco_root, split='train', ncap_per_img=args.ncap_per_img)\n",
    "print('[DEBUG] Loading train data ... %f secs' % (time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3bb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(dataset=train_data, num_workers=0, batch_size=args.batchsize, \\\n",
    "                               shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238f4f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16Feats(\n",
       "  (features_nopool): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "  )\n",
       "  (features_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imgcnn = Vgg16Feats()\n",
    "model_imgcnn.cuda()\n",
    "model_imgcnn.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1045204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convcap(\n",
       "  (emb_0): Embedding(9221, 512, padding_idx=0)\n",
       "  (emb_1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (imgproj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (resproj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (1): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (2): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "  )\n",
       "  (attention): ModuleList(\n",
       "    (0): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_0): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (classifier_1): Linear(in_features=256, out_features=9221, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convcap model\n",
    "model_convcap = convcap(train_data.numwords, args.num_layers, is_attention=args.attention)\n",
    "model_convcap.cuda()\n",
    "model_convcap.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97ce59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model_convcap.parameters(), lr=args.learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=.1)\n",
    "img_optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3353b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = args.batchsize\n",
    "ncap_per_img = args.ncap_per_img\n",
    "batchsize_cap = batchsize*ncap_per_img\n",
    "max_tokens = train_data.max_tokens\n",
    "nbatches = np.int_(np.floor((len(train_data.ids)*1.)/batchsize)) \n",
    "bestscore = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1e0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_img_per_cap(imgsfeats, imgsfc7, ncap_per_img):\n",
    "    batchsize, featdim, feat_h, feat_w = imgsfeats.size()\n",
    "    batchsize_cap = batchsize*ncap_per_img\n",
    "    imgsfeats = imgsfeats.unsqueeze(1).expand(batchsize, ncap_per_img, featdim, feat_h, feat_w)\n",
    "    imgsfeats = imgsfeats.contiguous().view(batchsize_cap, featdim, feat_h, feat_w)\n",
    "    \n",
    "    batchsize, featdim = imgsfc7.size()\n",
    "    batchsize_cap = batchsize*ncap_per_img\n",
    "    imgsfc7 = imgsfc7.unsqueeze(1).expand(batchsize, ncap_per_img, featdim)\n",
    "    imgsfc7 = imgsfc7.contiguous().view(batchsize_cap, featdim)\n",
    "    \n",
    "    return imgsfeats, imgsfc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68b32322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad88204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   for epoch in range(args.epochs):\n",
    "# 코드가 잘 돌아가는지 확인하기 위해 2번만 돌려봤습니다. 전 30(args.epochs)번 돌렸습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd78c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dbwls\\anaconda3\\envs\\convcap\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  0%|                                                                                        | 0/5664 [00:00<?, ?it/s]D:\\CVclass\\convcap-master\\convcap-master\\convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      " 18%|█████████████▎                                                              | 994/5664 [25:38<1:45:59,  1.36s/it]"
     ]
    }
   ],
   "source": [
    "  for epoch in range(2): \n",
    "    loss_train = 0.\n",
    "    \n",
    "    if(epoch == args.finetune_after):\n",
    "      img_optimizer = optim.RMSprop(model_imgcnn.parameters(), lr=1e-5)\n",
    "      img_scheduler = lr_scheduler.StepLR(img_optimizer, step_size=args.lr_step_size, gamma=.1)\n",
    "\n",
    "    scheduler.step()    \n",
    "    if(img_optimizer):\n",
    "      img_scheduler.step()\n",
    "\n",
    "    #One epoch of train\n",
    "    for batch_idx, (imgs, captions, wordclass, mask, _) in \\\n",
    "      tqdm(enumerate(train_data_loader), total=nbatches):\n",
    "\n",
    "      imgs = imgs.view(batchsize, 3, 224, 224)\n",
    "      wordclass = wordclass.view(batchsize_cap, max_tokens)\n",
    "      mask = mask.view(batchsize_cap, max_tokens)\n",
    "\n",
    "      imgs_v = Variable(imgs).cuda()\n",
    "      wordclass_v = Variable(wordclass).cuda()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      if(img_optimizer):\n",
    "        img_optimizer.zero_grad() \n",
    "\n",
    "      imgsfeats, imgsfc7 = model_imgcnn(imgs_v)\n",
    "      imgsfeats, imgsfc7 = repeat_img_per_cap(imgsfeats, imgsfc7, ncap_per_img)\n",
    "      _, _, feat_h, feat_w = imgsfeats.size()\n",
    "\n",
    "      if(args.attention == True):\n",
    "        wordact, attn = model_convcap(imgsfeats, imgsfc7, wordclass_v)\n",
    "        attn = attn.view(batchsize_cap, max_tokens, feat_h, feat_w)\n",
    "      else:\n",
    "        wordact, _ = model_convcap(imgsfeats, imgsfc7, wordclass_v)\n",
    "\n",
    "      wordact = wordact[:,:,:-1]\n",
    "      wordclass_v = wordclass_v[:,1:]\n",
    "      mask = mask[:,1:].contiguous()\n",
    "\n",
    "      wordact_t = wordact.permute(0, 2, 1).contiguous().view(\\\n",
    "        batchsize_cap*(max_tokens-1), -1)\n",
    "      wordclass_t = wordclass_v.contiguous().view(\\\n",
    "        batchsize_cap*(max_tokens-1), 1)\n",
    "      \n",
    "      maskids = torch.nonzero(mask.view(-1)).numpy().reshape(-1)\n",
    "\n",
    "      if(args.attention == True):\n",
    "        #Cross-entropy loss and attention loss of Show, Attend and Tell\n",
    "        loss = F.cross_entropy(wordact_t[maskids, ...], \\\n",
    "          wordclass_t[maskids, ...].contiguous().view(maskids.shape[0])) \\\n",
    "          + (torch.sum(torch.pow(1. - torch.sum(attn, 1), 2)))\\\n",
    "          /(batchsize_cap*feat_h*feat_w)\n",
    "      else:\n",
    "        loss = F.cross_entropy(wordact_t[maskids, ...], \\\n",
    "          wordclass_t[maskids, ...].contiguous().view(maskids.shape[0]))\n",
    "\n",
    "      loss_train = loss_train + loss.data\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      if(img_optimizer):\n",
    "        img_optimizer.step()\n",
    "\n",
    "    loss_train = (loss_train*1.)/(batch_idx)\n",
    "    print('[DEBUG] Training epoch %d has loss %f' % (epoch, loss_train))\n",
    "\n",
    "    modelfn = osp.join(args.model_dir, 'model.pth')\n",
    "\n",
    "    if(img_optimizer):\n",
    "      img_optimizer_dict = img_optimizer.state_dict()\n",
    "    else:\n",
    "      img_optimizer_dict = None\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model_convcap.state_dict(),\n",
    "        'img_state_dict': model_imgcnn.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        'img_optimizer' : img_optimizer_dict,\n",
    "      }, modelfn)\n",
    "\n",
    "    #Run on validation and obtain score\n",
    "    scores = test(args, 'val', model_convcap=model_convcap, model_imgcnn=model_imgcnn)\n",
    "    score = scores[0][args.score_select]\n",
    "\n",
    "    if(score > bestscore):\n",
    "      bestscore = score\n",
    "      print('[DEBUG] Saving model at epoch %d with %s score of %f'\\\n",
    "        % (epoch, args.score_select, score))\n",
    "      bestmodelfn = osp.join(args.model_dir, 'bestmodel.pth')\n",
    "      os.system('cp %s %s' % (modelfn, bestmodelfn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7331abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfn = osp.join(args.model_dir, 'bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73dba20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (osp.exists(bestmodelfn)):\n",
      "if (args.beam_size == 1):\n",
      "Loading annotation file...\n",
      "split_file : ./data/coco/annotations/dataset_coco_test.json\n",
      "Found 5000 images in split: test\n",
      "[DEBUG] #words in wordlist: 9221\n",
      "[DEBUG] Loading test data ... 0.113024 secs\n",
      "[DEBUG] Running inference on test with 250 batches\n",
      "[DEBUG] Loading checkpoint output\\bestmodel.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/250 [00:00<?, ?it/s]C:\\Users\\dbwls\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "D:\\imagecaptioning\\convcap-master\\convcap-master\\convcap.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "D:\\imagecaptioning\\convcap-master\\convcap-master\\test.py:86: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  wordprobs = F.softmax(wordact_t).cpu().data.numpy()\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 250/250 [00:37<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.327027\n",
      "creating index...\n",
      "index created!\n",
      "Using 5000/5000 predictions\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 48277, 'reflen': 47887, 'guess': [48277, 43277, 38277, 33277], 'correct': [34211, 17643, 7980, 3539]}\n",
      "ratio:1.008144\n",
      "Bleu_1: 0.709\n",
      "Bleu_2: 0.537\n",
      "Bleu_3: 0.392\n",
      "Bleu_4: 0.283\n",
      "computing METEOR score...\n",
      "METEOR: 0.243\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.520\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.897\n",
      "computing SPICE score...\n",
      "SPICE: 0.174\n",
      "TEST set scores\n",
      "Bleu_1: 0.708640\n",
      "Bleu_2: 0.537490\n",
      "Bleu_3: 0.391984\n",
      "Bleu_4: 0.282902\n",
      "METEOR: 0.242556\n",
      "ROUGE_L: 0.520152\n",
      "CIDEr: 0.897109\n",
      "SPICE: 0.173894\n"
     ]
    }
   ],
   "source": [
    "if (osp.exists(bestmodelfn)):\n",
    "    print('if (osp.exists(bestmodelfn)):')\n",
    "    \n",
    "    if (args.beam_size == 1):\n",
    "        print('if (args.beam_size == 1):')\n",
    "        scores = test(args, 'test', modelfn=bestmodelfn)\n",
    "    else:\n",
    "        print('else:')\n",
    "        scores = test_beam(args, 'test', modelfn=bestmodelfn)\n",
    "        \n",
    "    print('TEST set scores')\n",
    "    for k, v in scores[0].items():\n",
    "        print('%s: %f' % (k, v))\n",
    "else:\n",
    "    print('2 else')\n",
    "    raise Exception('No checkpoint found %s' % bestmodelfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f174bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Bleu_1', 0.5590387777249914), ('Bleu_2', 0.35789977257222577), ('Bleu_3', 0.2110013372472394), ('Bleu_4', 0.11876552651453906), ('METEOR', 0.1508381365015866), ('ROUGE_L', 0.41329740747549515), ('CIDEr', 0.3135012741095102), ('SPICE', 0.0854207356815281)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e00f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
