{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb5806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pickle \n",
    "import time\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    " \n",
    "from torchvision import models                                                                     \n",
    "from convcap import convcap\n",
    "from vggfeats import Vgg16Feats\n",
    "from coco_loader import Scale\n",
    "from PIL import Image\n",
    "from test_beam import repeat_img\n",
    "from beamsearch import beamsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cec401",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Convolutional Image \\\n",
    "    Captioning Model -- Caption Me')\n",
    "\n",
    "parser.add_argument('model_dir', help='output directory to save models & results')\n",
    "# parser.add_argument('image_dir', help='directory containing input images \\\n",
    "#                     supported formats .png, .jpg, .jpeg, .JPG')\n",
    "\n",
    "parser.add_argument('-g', '--gpu', type=int, default=0,\\\n",
    "                    help='gpu device id')\n",
    "\n",
    "parser.add_argument('--beam_size', type=int, default=1, \\\n",
    "                    help='beam size to use to generate captions') \n",
    "\n",
    "parser.add_argument('--attention', dest='attention', action='store_true', \\\n",
    "                    help='set caption model with attention in use (by default set)')\n",
    "\n",
    "parser.add_argument('--no-attention', dest='attention', action='store_false', \\\n",
    "                    help='set caption model without attention in use')\n",
    "\n",
    "parser.set_defaults(attention=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2593a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Convolutional Image \\\n",
    "    Captioning Model -- Caption Me')\n",
    "\n",
    "parser.add_argument('model_dir', help='output directory to save models & results')\n",
    "\n",
    "# parser.add_argument('image_dir', help='directory containing input images \\\n",
    "#                     supported formats .png, .jpg, .jpeg, .JPG')\n",
    "\n",
    "parser.add_argument('-g', '--gpu', type=int, default=0,\\\n",
    "                    help='gpu device id')\n",
    "\n",
    "parser.add_argument('--beam_size', type=int, default=1, \\\n",
    "                    help='beam size to use to generate captions') \n",
    "\n",
    "parser.add_argument('--attention', dest='attention', action='store_true', \\\n",
    "                    help='set caption model with attention in use (by default set)')\n",
    "\n",
    "parser.add_argument('--no-attention', dest='attention', action='store_false', \\\n",
    "                    help='set caption model without attention in use')\n",
    "\n",
    "parser.set_defaults(attention=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56972084",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef314a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelfn = osp.join('output/bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec2f554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/bestmodel.pth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodelfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99772866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_dir):\n",
    "    exts = ['.jpg', '.jpeg', '.png']\n",
    "    imgs = torch.FloatTensor(torch.zeros(0, 3, 224, 224))\n",
    "    imgs_fn = []\n",
    "\n",
    "    img_transforms = transforms.Compose([\n",
    "        Scale([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], \n",
    "        std = [ 0.229, 0.224, 0.225 ])\n",
    "    ])\n",
    "    \n",
    "    for fn in os.listdir(image_dir):\n",
    "        if(osp.splitext(fn)[-1].lower() in exts):\n",
    "            imgs_fn.append(os.path.join(image_dir, fn))\n",
    "            img = Image.open(os.path.join(image_dir, fn)).convert('RGB')\n",
    "            img = img_transforms(img)\n",
    "            imgs = torch.cat([imgs, img.unsqueeze(0)], 0)\n",
    "\n",
    "    return imgs, imgs_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf632b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, imgs_fn = load_images('my_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cdcb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For trained model released with the code\n",
    "batchsize = 1\n",
    "max_tokens = 15\n",
    "num_layers = 3 \n",
    "worddict_tmp = pickle.load(open('data/wordlist.p', 'rb'))\n",
    "wordlist = [l for l in iter(worddict_tmp.keys()) if l != '</S>']\n",
    "wordlist = ['EOS'] + sorted(wordlist)\n",
    "numwords = len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d9010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading checkpoint output/bestmodel.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "convcap(\n",
       "  (emb_0): Embedding(9221, 512, padding_idx=0)\n",
       "  (emb_1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (imgproj): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (resproj): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (1): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "    (2): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(4,))\n",
       "  )\n",
       "  (attention): ModuleList(\n",
       "    (0): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): AttentionLayer(\n",
       "      (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_0): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (classifier_1): Linear(in_features=256, out_features=9221, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imgcnn = Vgg16Feats()\n",
    "model_imgcnn.cuda() \n",
    "\n",
    "model_convcap = convcap(numwords, num_layers, is_attention = args.attention)\n",
    "model_convcap.cuda()\n",
    "\n",
    "print('[DEBUG] Loading checkpoint %s' % bestmodelfn)\n",
    "checkpoint = torch.load(bestmodelfn)\n",
    "model_convcap.load_state_dict(checkpoint['state_dict'])\n",
    "model_imgcnn.load_state_dict(checkpoint['img_state_dict'])\n",
    "\n",
    "model_imgcnn.train(False) \n",
    "model_convcap.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b217236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/10 [00:00<?, ?it/s]D:\\imagecaptioning\\convcap-master\\convcap-master\\convcap.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(sz[0] * sz[1], sz[2]))\n",
      "D:\\imagecaptioning\\convcap-master\\convcap-master\\beamsearch.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  beam_word_logprobs = self.logsoftmax(output).cpu().data.tolist()\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_captions = []\n",
    "for batch_idx, (img_fn) in tqdm(enumerate(imgs_fn), total=len(imgs_fn)):\n",
    "    \n",
    "    img = imgs[batch_idx, ...].view(batchsize, 3, 224, 224)\n",
    "\n",
    "    img_v = Variable(img.cuda())\n",
    "    imgfeats, imgfc7 = model_imgcnn(img_v)\n",
    "\n",
    "    b, f_dim, f_h, f_w = imgfeats.size()\n",
    "    imgfeats = imgfeats.unsqueeze(1).expand(b, args.beam_size, f_dim, f_h, f_w)\n",
    "    imgfeats = imgfeats.contiguous().view(b*args.beam_size, f_dim, f_h, f_w)\n",
    "\n",
    "    b, f_dim = imgfc7.size()\n",
    "    imgfc7 = imgfc7.unsqueeze(1).expand(b, args.beam_size, f_dim)\n",
    "    imgfc7 = imgfc7.contiguous().view(b*args.beam_size, f_dim)\n",
    "    \n",
    "    beam_searcher = beamsearch(args.beam_size, batchsize, max_tokens)\n",
    "\n",
    "    wordclass_feed = np.zeros((args.beam_size*batchsize, max_tokens), dtype='int64')\n",
    "    wordclass_feed[:,0] = wordlist.index('<S>') \n",
    "    outcaps = np.empty((batchsize, 0)).tolist()\n",
    "    \n",
    "    \n",
    "    for j in range(max_tokens-1):\n",
    "        wordclass = Variable(torch.from_numpy(wordclass_feed)).cuda()\n",
    "\n",
    "        wordact, attn = model_convcap(imgfeats, imgfc7, wordclass)\n",
    "        wordact = wordact[:,:,:-1]\n",
    "        wordact_j = wordact[..., j]\n",
    "\n",
    "        beam_indices, wordclass_indices = beam_searcher.expand_beam(wordact_j)  \n",
    "\n",
    "        if len(beam_indices) == 0 or j == (max_tokens-2): # Beam search is over.\n",
    "            generated_captions = beam_searcher.get_results()\n",
    "            for k in range(batchsize):\n",
    "                g = generated_captions[:, k]\n",
    "                outcaps[k] = [wordlist[x] for x in g]\n",
    "        else:\n",
    "            wordclass_feed = wordclass_feed[beam_indices]\n",
    "            imgfc7 = imgfc7.index_select(0, Variable(torch.cuda.LongTensor(beam_indices)))\n",
    "            imgfeats = imgfeats.index_select(0, Variable(torch.cuda.LongTensor(beam_indices)))\n",
    "            for i, wordclass_idx in enumerate(wordclass_indices):\n",
    "                wordclass_feed[i, j+1] = wordclass_idx\n",
    "                \n",
    "    for j in range(batchsize):\n",
    "        num_words = len(outcaps[j])\n",
    "        if 'EOS' in outcaps[j]:\n",
    "            num_words = outcaps[j].index('EOS')\n",
    "        outcap = ' '.join(outcaps[j][:num_words])\n",
    "        pred_captions.append({'img_fn': img_fn, 'caption': outcap})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e625f31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'img_fn': 'my_image\\\\image1.jpg',\n",
       "  'caption': 'a group of people standing on top of a mountain'},\n",
       " {'img_fn': 'my_image\\\\image10.jpg',\n",
       "  'caption': 'two men are drinking from a white table'},\n",
       " {'img_fn': 'my_image\\\\image2.jpg',\n",
       "  'caption': 'a cat sitting on a ground next to a dead tree'},\n",
       " {'img_fn': 'my_image\\\\image3.jpg',\n",
       "  'caption': 'a group of people sitting around a table with a laptop'},\n",
       " {'img_fn': 'my_image\\\\image4.jpg',\n",
       "  'caption': 'a large building with a large building in the background'},\n",
       " {'img_fn': 'my_image\\\\image5.jpg',\n",
       "  'caption': 'a plate of food with a fork and a fork'},\n",
       " {'img_fn': 'my_image\\\\image6.jpg',\n",
       "  'caption': 'a table with a bunch of different types of items'},\n",
       " {'img_fn': 'my_image\\\\image7.jpg',\n",
       "  'caption': 'a busy city street with cars and cars'},\n",
       " {'img_fn': 'my_image\\\\image8.jpg',\n",
       "  'caption': 'two brown bears are sitting on a tree'},\n",
       " {'img_fn': 'my_image\\\\image9.jpg',\n",
       "  'caption': 'a man is standing next to a large elephant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "442c7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "resfile = osp.join('my_image', 'captions.txt')\n",
    "with open(resfile, 'w') as fp:\n",
    "    for item in pred_captions:\n",
    "        fp.write('image: %s, caption: %s\\n' % (item['img_fn'], item['caption']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba273539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
